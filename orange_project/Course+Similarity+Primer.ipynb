{"cells": [{"cell_type": "markdown", "source": "# Measuring Course Similiarity \n\nThis document will demonstrate to quantify similiarity of different courses within the Orange course offerings.  The\nsimiliarity of courses will be measured by comparing the similiarity of terms within the course descriptions.  The\nfirst order implementation of this involves identifying all words within the course description and measuring which course have \na large intersection of terms.  However this simple model has several shortcomings:\n\n1) A large number of words are ubiquitous and uninformative.  Articles, pronouns, and conjunctions do not carry \nsignificant computational information on the content and audience of an article\n\n2) There is significant ambiguity within language involving conjucations and declension.  For example \"computer\" and \n\"computers\" identify a single concept, but  within the 'bag-of-words' model described above these concepts are not linked.\n\n3) Language is sparse and and word can be highly correlated.  For example terms like \"SQL\", \"HIVE\", and \"Database\" are \nall related and courses which discuss each may be similiar.  But using a simple bag-of-words model will ignore these \nrelationships and lose correlated information.\n\nThis document will attempt to show how each of these shortcomings can be addressed within a similiarity measurement \nsystem.\n\nAs such these document is broken into the following sections:\n\n\n1) Environment setup: Loading required packages which may need to be installed\n\n2) Ingestion: Loading the data into Jupyter\n\n3) Tokenization, Cleansing, and Stemming: Identifying words within the narrative, removing uninformative words, \nand disambiguating word tense.\n\n4) Latent Semantic Index: Addressing highly correlated terms and reducing language dimensionality\n\n5) Similiarity Measurement: Measuring course similiarity\n\n6) Storage and reuse", "metadata": {}}, {"cell_type": "markdown", "source": "## Environment Setup\n\nTwo non-standard python packages are required for this analysis:\n\n* NLTK: *Natural Language Toolkit* highly advanced NLP toolkit.  We will use it to disambiguate tense, known as stemming.  \nHowever the package has several features which are useful to explore including part of speech tagging and existing \ntext corpora.  Part of Speech (POS) tagging is useful in identify noun phrases within a narrative.  Linguistically \nnoun phrases often carry a great deal a information when compared to verb phrases.  This may later help reduce \ndimensionality\n\n* STOP_WORDS: A simple package containing uninformative english words like conjucations, pronouns, etc.  We will use this \nto remove these words from the matrix and reduce dimensionality", "metadata": {}}, {"cell_type": "code", "source": "%matplotlib inline", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "trusted": true}}, {"cell_type": "code", "source": "import pip\nrequired_packages = ['nltk', 'stop_words']\ninstalled_packages = [package.project_name for package in pip.get_installed_distributions()]\nfor pkg_name in required_packages:\n    if not pkg_name in installed_packages:\n        pip.main(['install', pkg_name])", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "import stop_words\nfrom nltk.stem import PorterStemmer\nimport stop_words\nfrom IPython.display import *\nfrom matplotlib.pylab import *\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nimport re\nfrom scipy.sparse import linalg as sla\nfrom functools import partial\nimport sqlalchemy as sq\nimport pandas as pd\nimport numpy as np\nimport sklearn.cross_validation as cv\nimport getpass", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "## Ingestion\n\nWe connect to the source database and load the course information", "metadata": {}}, {"cell_type": "code", "source": "course_data = %sql SELECT * FROM course_description_catalog\ndf = course_data.DataFrame()", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "df[df.series.apply(lambda ele: 'securi' in ele.lower())]", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "\n## Tokenization, Cleansing, and Stemming\n\nWe need to take the text of the course description and break it down into individual words (tokenization), \nremove words which are uninformative (cleansing or stop word removal), \nand identify the common roots of identified words (stemming).\n", "metadata": {}}, {"cell_type": "code", "source": "example_narrative = df.iloc[0]['coursedes']\nprint(example_narrative)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# tokenization: we split words if their exists a space or a limited set of puncations {-, !, ?, :, .,  ;, %, (, )}\n# This leads to some mistakes (for example E-mail is broken into e and mail).\nelements = re.split(\"[, (\\-!?:.;%'\\\")]+\", example_narrative.lower())\nelements", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# stop word removal is simply a list of words.  it is important to remove in a bag-o-words model to reduce dimensionality.\n# if n-gram pharses are used later to capture negation or other concepts, this needs to be done more carefully\nstopwords = stop_words.get_stop_words('english')\nstopwords[0:10]", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "elements_prime = [x for x in elements if not x in stopwords]\nprint(\"Cleansed list: %s\"%elements)\nprint(\"Removed Words: %s\"%set.difference(set(elements), set(elements_prime)))", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Stemming: we use a simple stemming algorithm from NLTK to find the roots of words:\nstemmer = PorterStemmer()\nelements_stemmed = [stemmer.stem(x) for x in elements_prime ]\nprint(elements_stemmed)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Defining a tokenization function\nstopwords = stop_words.get_stop_words('english')\nstemmer = PorterStemmer()\n\ndef tokenizer(narrative, stopwords=None, stemmer=None):\n    elements = re.split(\"[, (\\-!?:.;%'\\\")]+\", narrative.lower())\n    elements = [x for x in elements if len(x) > 0]\n    if stopwords is not None:\n        elements = [x for x in elements if not x in stopwords]    \n    if stemmer is not None:\n        elements = [stemmer.stem(x) for x in elements]\n    return(elements)\ntokenizer_fcn = partial(tokenizer,stopwords=stopwords, stemmer=stemmer)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "tokenizer_fcn(example_narrative)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "### Building out the TF and TF-IDF Matrix\n\nWe can use sklearn's count tokenizer to create the initial Term Frequency matrix and the TfidfTransformer \nto transform this into an TF-IDF matrix.  This will serve as the basis to measure similiarity going forward.", "metadata": {}}, {"cell_type": "code", "source": "count_mdl = CountVectorizer(tokenizer=tokenizer_fcn)\ntf_mtx = count_mdl.fit_transform(df.coursedes)\ntf_idf_fcn = TfidfTransformer(norm=None)\ntf_idf_mtx = tf_idf_fcn.fit_transform(tf_mtx)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Build the TF dataframe\ncourse_index, word_index = tf_mtx.nonzero()\ncount = tf_mtx.data\ndf_term_frequency = pd.DataFrame({\"course_id\": df['course#'].values[course_index], \n                                 \"word\": np.array(count_mdl.get_feature_names())[word_index], \n                                 'count': count})\nHTML(df_term_frequency.head(n=20).to_html())", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Load at the most prominent words in the corpus\ndf_term_frequency.groupby(\"word\").sum().sort_values('count', ascending=False).head(n=20)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "## Latent Semantic Analysis\n\nBefore we measure similiarity we want to reduce dimensionality. Both the TF and TF-IDF matrices can be approximated using \na reduced rank matrix:\n\n$$TFIDF = U S V^{T} \\approx U_{k} S_{k} V_{k}^{T} $$\n\nWhere $U_{k}$, $S_{k}$, and $V_{k}$ are truncated terms in the SVD decomposition focusing on the basis vectors \nwhich explain most the variance in the dataset.  In this expression we can reduce dimensionality and focus on \n\nwhere we explain ", "metadata": {}}, {"cell_type": "code", "source": "U, s, V = sla.svds(tf_idf_mtx, k=500)\nplot(np.arange(500), s[-1::-1])\n", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# The singular values plataue at about 50 terms.  For simplicity we will truncate the SVD there", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "latent_course_vectors_k, s, latent_word_vectors_k = sla.svds(tf_idf_mtx, k=50)\nlatent_word_vectors_k = latent_word_vectors_k.T", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "## Similiarity measurement\n\nWe can now measure similiarity between courses using the latent course vectors and the cosine similiairty ", "metadata": {}}, {"cell_type": "code", "source": "from sklearn.metrics.pairwise import cosine_similarity", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Remove the diagonal such that the course is not similiar to itself\nsimiliarity_matrix = cosine_similarity(latent_course_vectors_k) - np.identity(latent_course_vectors_k.shape[0])", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Let's look at one course:\nidx = np.random.choice(np.arange(df.shape[0]))\nselected_course = df.ix[idx]\nselected_course", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "values = similiarity_matrix[idx, :]\nsimiliar_courses = values.argsort()[-10::]\nvalues.sort()\nselect_values = values[-10::]\ndf_tmp = pd.DataFrame({\"course title\" : df.iloc[similiar_courses[::-1]]['course title'], \n                      'similiarity': select_values[::-1]})\nHTML(df_tmp.to_html())", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "## Let's look at the histogram of similiarities\nresults = hist(np.array(similiarity_matrix.flatten()), np.linspace(-1, 1, 200))\nxlabel('Similiarity')\nylabel('Counts/(0.01)')", "outputs": [], "execution_count": null, "metadata": {"scrolled": true, "collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Let's see what the top 2.5% of similiar courses are and save those are similiar in \n# a database\nii, jj = np.triu_indices(similiarity_matrix.shape[0], k=1)\nthresh = np.percentile(similiarity_matrix[ii, jj], 97.5)\nprint(thresh)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "similiarity_matrix[similiarity_matrix < thresh] = 0\nii, jj = np.tril_indices(similiarity_matrix.shape[0], k=-1)\nsimiliarity_matrix[ii, jj] = 0", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "sp_similiarity_matrix = sparse.csr_matrix(similiarity_matrix)\n", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# We are double counting her\nii, jj = sp_similiarity_matrix.nonzero()\nsimiliarities = sp_similiarity_matrix.data\ncourse_id_1 = df['course#'].values[ii]\ncourse_id_2 = df['course#'].values[jj]\ncourse_title_1 = df['course title'].values[ii]\ncourse_title_2 = df['course title'].values[jj]", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "df_similiarity = pd.DataFrame({'course_number_1': course_id_1, \n                              'course_number_2': course_id_2, 'course title 1': course_title_1, \n                               'course title 2': course_title_2,\n                              'similiarity': similiarities})\ndf_similiarity.sort_values('similiarity', ascending=False, inplace=True)\nHTML(df_similiarity.head(n=50).to_html())", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "There are a host of test prep courses which filter to the top", "metadata": {"collapsed": false}}, {"cell_type": "code", "source": "df_similiarity_tmp = pd.merge(df_similiarity, df, left_on = 'course_number_1', right_on = 'course#', how='inner')", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "trusted": true}}, {"cell_type": "code", "source": "HTML(df_similiarity_tmp[df_similiarity_tmp.series != 'Test Preps'][['course title 1', 'course title 2', 'similiarity']].sort_values('similiarity', ascending=False).head(n=20).to_html())", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "# Check that each course is similiar to at least one other course\nset.difference(set(np.arange(similiarity_matrix.shape[0])), set.union(set(np.unique(ii)), set(np.unique(jj))))", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "markdown", "source": "## Storage and Reuse\n\nNow we would like to write out some of these table to the source database so that they can be reused later.\n\nWe specifically want:\n\n* TF Matrix: This will be useful if we ever want to recompute the TF-IDF matrix and redo the latent semantic\nfactorization\n\n* Document Latent Factors: This will be useful if we want to calculate similairities later\n\n* Word Latent Factors: Useful in implementing the fold-in method to update the document latent factors without\nrecalculating the factorization\n\n* Similiarity Table: Useful in identifying similiar courses", "metadata": {}}, {"cell_type": "code", "source": "password = getpass.getpass()\nconn = sq.create_engine('postgresql://hr:%s@192.168.161.79:5432/hr?sslmode=require'%password)\ndel(password)", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "df_term_frequency.to_sql('course_term_frequency_tbl', conn, if_exists='replace')", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "df_similiarity.to_sql('course_similiarity_tbl', conn, if_exists='replace')", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "trusted": true}}, {"cell_type": "code", "source": "df_latent_course_vectors = pd.DataFrame(latent_course_vectors_k, index=df['course#'].values, columns = ['latent_vector%i'%ii for ii in range(latent_course_vectors_k.shape[1])])\ndf_latent_word_vectors = pd.DataFrame(latent_word_vectors_k, index=count_mdl.get_feature_names(), columns = ['latent_vector%i'%ii for ii in range(latent_word_vectors_k.shape[1])])\ndf_latent_singular_values = pd.DataFrame({\"name\": ['latent_vector%i'%ii for ii in range(latent_word_vectors_k.shape[1])], 'value': s})", "outputs": [], "execution_count": null, "metadata": {"collapsed": false, "trusted": true}}, {"cell_type": "code", "source": "df_latent_course_vectors.to_sql('latent_course_vector_tbl', conn, if_exists='replace')\ndf_latent_word_vectors.to_sql('latent_word_vector_tbl', conn, if_exists='replace')\ndf_latent_singular_values.to_sql('latent_singular_values_tbl', conn, if_exists='replace')", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "trusted": true}}, {"cell_type": "code", "source": "", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "trusted": true}}], "nbformat_minor": 0, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.5.2", "nbconvert_exporter": "python", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "_immuta": {"allow_rename": false}}, "nbformat": 4}