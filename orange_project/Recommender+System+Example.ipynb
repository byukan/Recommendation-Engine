{"cells": [{"execution_count": 3, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport patsy\nfrom matplotlib.pylab import *\nimport os\nfrom os import path\nfrom IPython.display import *\nfrom sklearn import cross_validation\nfrom sklearn import svm\nfrom sklearn import linear_model\nfrom scipy.sparse import linalg as la\nfrom IPython.display import *\nimport tensorflow as tf\nimport multiprocessing as mp\nfrom scipy import sparse\nfrom scipy.sparse import linalg as sla", "outputs": []}, {"metadata": {}, "source": "## User Based Filtering Development\n\nFor this analysis we want to attempt to define a mathematical model to predict the user's rating of unknown \nmovies.  The first attempt will be to look at a simple user based filtering approach.  \nThis approach looks at correlations in ratings of movies \nbetween the ratings of individual users.  To start out we consider a simple model consisting of three \nterms:\n\n* Average total purchase made by a user\n\n* Average total purchase made at the store\n\n* Latent factors which correlate user ratings and movies (This will be discussed more later)\n\nTo create this model we start with the following expression:\n\n$$ P(User, Movie) = \n\\alpha_{U} \\times \\overline{R(User)} + \n\\alpha_{N} \\times \\overline{R(Movie)} + \nF_{SVD}(User, Movie| \\lambda, N_{Rank}) $$\n\nThis expression defines a baseline purchase as simply the mean purchase amount at each store\n, $\\overline{R(Movie)}$, in a weighted average with the mean purchase amount at a customer\n, $\\overline{R(User)}$.  The term, $F_{SVD}(User, Movie| \\lambda, N_{Rank})$\nis a derived from Funk's SVD decomposition, used win Netflix's recommender system prize \n(This will be discussed in more detail shortly).", "cell_type": "markdown"}, {"execution_count": 4, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "## Get the data\nsource_file = 'ExampleData/ml-1m.zip'\nif not os.path.isdir(\"ExampleData\"):\n    os.mkdir(\"ExampleData\")\nif not os.path.isfile(\"ExampleData/ml-1m.zip\"):\n    from urllib.request import urlretrieve\n    urlretrieve('http://files.grouplens.org/datasets/movielens/ml-1m.zip', source_file)\nimport zipfile\nfid = zipfile.ZipFile(source_file,'r')\nfid.extractall('ExampleData')", "outputs": []}, {"execution_count": 5, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "data_directory = 'ExampleData/ml-1m/'\ndf_ratings = pd.read_csv(path.join(data_directory, 'ratings.dat'), header=None, sep=\"::\")\ndf_ratings.columns = ['user_id', 'movie_id', 'rating', 'time']\n\n# If any movie or user has fewer than 20 entries drop them\nmin_count = 20\nuser_counts = df_ratings['user_id'].value_counts() \nuser_ids = user_counts[user_counts > min_count].index\nmovie_counts = df_ratings['movie_id'].value_counts()\nmovie_ids = movie_counts[movie_counts > min_count].index\n\nmovie_ok = df_ratings.movie_id.isin(movie_ids)\nuser_ok = df_ratings.user_id.isin(user_ids)\nall_ok = movie_ok & user_ok\n\ndf_ratings = df_ratings[all_ok].copy()", "outputs": [{"output_type": "stream", "text": "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  from ipykernel import kernelapp as app\n", "name": "stderr"}]}, {"execution_count": 6, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "HTML(df_ratings.head(n=10).to_html())", "outputs": [{"execution_count": 6, "metadata": {}, "output_type": "execute_result", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>1197</td>\n      <td>3</td>\n      <td>978302268</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>1287</td>\n      <td>5</td>\n      <td>978302039</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>2804</td>\n      <td>5</td>\n      <td>978300719</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>594</td>\n      <td>4</td>\n      <td>978302268</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>919</td>\n      <td>4</td>\n      <td>978301368</td>\n    </tr>\n  </tbody>\n</table>"}}]}, {"metadata": {}, "source": "## User and Movie Average Ratings\n\nPrehaps the most niave model is to simply look at the recommendation as the mean rating \ngiven by a user and given to a movie.  This simple approach has several appeals:\n\n1) Both the average movie rating and the average user rating can be measured with relatively small samples (<50)\n\n2) The number of free parameters is small, reducing the risk of over training\n\n3) Easily fit and stable to new observations\n\nThis first section will show only fitting these two parameters.\n\nPerformance will be assessed using 70/30 training and validation split.", "cell_type": "markdown"}, {"metadata": {}, "source": "### Algrebraic Manipulation\n\nThe formula shown earlier has an implicit constraints:\n\n$$ \\sum{\\alpha_{i}} = 1 $$\n\nWhile there are toolkits which handle these constraints, sklearn's \nlinear model fitting is unfortunately not one of them.  \nBut we back into these constraints with some simple linear algrebra:\n\n$$ P(User, Movie) - \\overline{P_{User}}  = \\alpha_{Movie}\n\\left(\\overline{P_{Movie}} - \\overline{P_{User}}\\right) $$\n\nThis means we will only be fitting for $\\alpha_{Movie}$ and will back into $\\alpha_{User} = 1-\\alpha_{Movie}$", "cell_type": "markdown"}, {"execution_count": 7, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "# Measure the mean ratings for per user and per movie\ndef GetMeanRatings(df):\n    mean_user_rating = df.groupby(\"user_id\")['rating'].mean()\n    mean_user_rating.name = 'mean_user_rating'\n    mean_movie_rating = df.groupby(\"movie_id\")['rating'].mean()\n    mean_movie_rating.name = 'mean_movie_rating'\n    return(mean_user_rating, mean_movie_rating)\n\n# Append the mean ratings to a data frame\ndef AppendMeanRatings(df, mean_user_ratings, mean_movie_ratings):\n    df_augment = df.copy()\n    df_augment['mean_user_rating'] = mean_user_ratings.loc[df.user_id].values\n    df_augment['mean_movie_rating'] = mean_movie_ratings.loc[df.movie_id].values\n    return(df_augment)\n\n# Create X, y for the linear model\ndef BuildPredictionModel(df):\n    X = df.mean_movie_rating - df.mean_user_rating\n    y = df.rating - df.mean_user_rating\n    X = X.values.reshape(-1, 1)\n    return(X, y)\n\n# Predict the ratings (y) from a trained linear model\ndef PredictModel(df, mdl):\n    X, _ = BuildPredictionModel(df)\n    delta_p = mdl.predict(X)\n    predict_rating = df.mean_user_rating + delta_p\n    return(predict_rating)\n\n# Run the training and prediction model\ndef TrainAndPredict(df_train, df_test):\n    mean_user_ratings, mean_movie_ratings = GetMeanRatings(df_train)\n    df_train_aug = AppendMeanRatings(df_train, mean_user_ratings, mean_movie_ratings)\n    df_test_aug = AppendMeanRatings(df_test, mean_user_ratings, mean_movie_ratings)\n    Xtrain, ytrain = BuildPredictionModel(df_train_aug)\n    mdl = linear_model.LinearRegression(fit_intercept=False)\n    mdl.fit(Xtrain, ytrain)\n    ypred = PredictModel(df_test_aug, mdl)\n    err = ypred - df_test_aug.rating\n    rmse = np.sqrt((err**2).mean())\n    return(rmse, mdl)\n\n# Execute the training/validation split\ndef SplitTrainingValidation(df, training_percentage = 0.70):\n    split = cross_validation.ShuffleSplit(df.shape[0], n_iter=1, test_size=1-training_percentage)\n    train_idx, test_idx = list(split)[0]\n    train_index = df.index[train_idx]\n    test_index = df.index[test_idx]\n    df_train = df.loc[train_index].copy()\n    user_ids = df_train.user_id.drop_duplicates()\n    movie_ids = df_train.movie_id.drop_duplicates()\n    df_test = df.loc[test_index].copy()\n    df_test = df_test[df_test.movie_id.isin(movie_ids) & \n                     df_test.user_id.isin(user_ids)].copy()\n    return(df_train, df_test)\n", "outputs": []}, {"execution_count": 8, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "## Look at simple model performance as a baseline\ndf_train, df_test = SplitTrainingValidation(df_ratings)\nrmse, mdl = TrainAndPredict(df_train, df_test)\nprint(\"RMSE = %f, alpha_{Movie}=%f\"%(rmse, mdl.coef_[0]))", "outputs": [{"output_type": "stream", "text": "RMSE = 0.952301, alpha_{Movie}=0.634819\n", "name": "stdout"}]}, {"metadata": {}, "source": "## Augmenting Model with Latent Factors\n\nThe winning Netflix algorithm start began by representing the movie rating matrix in the \nform $R_{Users, Movie}$, where the rows are the users and the columns are the movies, \n[citation](http://www.scielo.br/pdf/jistm/v13n3/1807-1775-jistm-13-03-0497.pdf).  If \nall users had rated all movies this matrix would be fulled measured.  In that case \nit would be reasonable to decompose the matrix using the familiar signular value decomposition (SVD), \n$R_{Users, Movies} = U \\Sigma V$, where $U$ and $V$ are square with the number of users and number\nof movies as the number of columns, respectively.  The columns of $U$ would represent linear\ncombinations of correlated users and the rows of $V$ would represent correlated movies.  If \n$R_{Users, Movies}$ was full know we could use the SVD to reduce the dimensionality of the space\nusing a Principal Component Analysis to approximate $R_{Users, Movies}$ as \nonly a product and identify highly correlated movies or users.  Unfortunately we \ndo not have a fully determined $R_{Users, Movies}$.\n\nFunk's algorithm instead took the available data we do have and used it to fit the following model:\n\n$$ R_{Users, Movies} = R_{Users, N} \\times R_{Movies, N}^{T} $$\n\nWhere $N$ is the number of latent basis vectors in the model, and all terms in \nboth  $R_{Users, N}$ and $R_{Movies, N}$ are unknown.  This means for a problem \nlike this example data we only have ~5.5% of the elements of $R_{Users, Movies}$.  This means\nwe need to start the problem with some approximations for the SVD.  Also rather than fitting \nthe ratings themselves, we will instead fit the residuals of the\nlinear model prediction,\n\nThe first we need a starting point for the algorithm's convergence.  To do this we will \nrun a SVD assuming $R_{Users, Movies}$ is fully determined, where $R_{Users, Movies}$ \nis the residual.  To do this we will look at two \ncases:\n\n1) If $R_{Users, Movies}$ is known (e.g. in the training data) residual between the linear \nmodel and the observed rating\n\n2) If $R_{Users, Movies}$ is unknown set it to zero\n\n", "cell_type": "markdown"}, {"execution_count": 9, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "mean_user_ratings, mean_movie_ratings = GetMeanRatings(df_train)\ndf_train_aug = AppendMeanRatings(df_train, mean_user_ratings, mean_movie_ratings)\npredicted_rating = PredictModel(df_train_aug, mdl)\ndf_train_aug['predicted_rating'] = predicted_rating\n\n# Measure the residual for the training data\ndf_train_aug['residual'] = df_train_aug.predicted_rating - df_train_aug.rating\n\n# Make a pivot table for the observed values\ntbl = df_train_aug.pivot_table(values='residual', columns = 'movie_id', \n                     index = 'user_id', aggfunc='sum', fill_value=0)\n# Transform the pivot table into a sparse matrix\nmtx = sparse.coo_matrix(tbl.as_matrix())\n\n# Solve for the leading (N=30) SVD values\nn_components = 30\nU, S, V = la.svds(mtx, k=n_components)\n\n# Set $R_{Users, N}$ to U \\time S^{1/2}\n# Set $R_{Movies, N}$ to V \\time S^{1/2}\ndf_users = pd.DataFrame(U.dot(np.diag(np.sqrt(S))), index=tbl.index)\ndf_movies = pd.DataFrame(np.diag(np.sqrt(S)).dot(V).T, index=tbl.columns)\ndf_users.columns =  ['ULatent_%s'%str(ii).zfill(2) for ii in range(n_components)]\ndf_movies.columns = ['MLatent_%s'%str(ii).zfill(2) for ii in range(n_components)]", "outputs": []}, {"metadata": {}, "source": "Now we can execute the following minimization:\n\n$$ Minimize~((R_{Users, Movies} - R_{Users, N} \\times R_{Movies, N}^{T}))^{2} + \n\\lambda \\left(||R_{Users, N}|| + ||R_{Movies, N}|| \\right) $$\n\nThis first expression is simply the Mean Absolute Error of the prediction.  The \nsecond expression is a regularization parameter, used to contrain the size of either \n$R_{Users, N}$ or $R_{Movies, N}$.\n\nPractically we will simply fix $R_{Users, N}$ and fit $R_{Movies, N}$.  Once that \nconverges we will fix $R_{Movies, N}$ to the optimal value and fit $R_{Users, N}$.", "cell_type": "markdown"}, {"execution_count": 10, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "\ndef FitLatentWeights(latent_row, index, dependent_column,\n                     df_complimentary, df_grp, complimentary_column, alpha):\n    df_sub = df_grp.get_group(index)\n    X = df_complimentary.loc[df_sub[complimentary_column]]\n    y = df_sub[dependent_column]\n    mdl = linear_model.SGDRegressor(loss='squared_loss', penalty='l1', alpha =alpha, fit_intercept=False, \n                                   n_iter = 100)\n    mdl.fit(X.as_matrix(), y.as_matrix())\n    coefs = mdl.coef_\n    df = pd.Series(coefs, index=latent_row.index.values)\n    return(df)\n\ndef PredictSVDRating(df, df_user_latent, df_movie_latent, predicted_rating_column):\n    p1 = df_user_latent.loc[df.user_id]\n    p2 = df_movie_latent.loc[df.movie_id]\n    p_prod = (p1.values * p2.values).sum(axis=1)\n    prediction = df[predicted_rating_column]-p_prod\n    return(prediction)", "outputs": []}, {"execution_count": 11, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "lambda_value =1e-2\ndf_user_grp = df_train_aug.groupby('user_id')\ndf_movie_grp = df_train_aug.groupby(\"movie_id\")\ndf_users_latent = df_users.apply(lambda row: FitLatentWeights(row, row.name, 'residual',\n                                                              df_movies, \n                                                              df_user_grp, 'movie_id', lambda_value), axis=1)\ndf_movie_latent = df_movies.apply(lambda row: FitLatentWeights(row, row.name, 'residual',\n                                                               df_users_latent, \n                                                               df_movie_grp, \n                                                               'user_id', lambda_value), axis=1)", "outputs": []}, {"execution_count": 12, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "df_test_aug = AppendMeanRatings(df_test, mean_user_ratings, mean_movie_ratings)\npredicted_rating = PredictModel(df_test_aug, mdl)\ndf_test_aug['predicted_rating'] = predicted_rating\ndf_test_aug['SVD Prediction'] = PredictSVDRating(df_test_aug, df_users_latent, df_movie_latent, 'predicted_rating')", "outputs": []}, {"execution_count": 13, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "# Look at RMSE for the model augmented with the SVD\nerr= df_test_aug['SVD Prediction'] - df_test_aug.rating\nprint(\"RMSE = %f\"%np.sqrt((err**2).mean()))", "outputs": [{"output_type": "stream", "text": "RMSE = 0.866730\n", "name": "stdout"}]}, {"metadata": {}, "source": "## Results\n\nOther recommender systems such as [Surpriselib](http://surpriselib.com) show smalled RMSEs than this (~0.70).  This \nis in part due to sklearn's Stochastic Gradient Solver using the\nthe squared loss function $((R_{Users, Movies} - R_{Users, N} \\times R_{Movies, N}^{T}))^{2}$, as \nopposed to the Mean Absolute Error used by Supriselib.  \nIn order to use this loss function either use Supriselib or\ncode the algorithm in Tensorflow, which allows more flexible loss function definitions. \n\nWe can make these changes going forward, but this shows a working framework.", "cell_type": "markdown"}, {"metadata": {}, "source": "## Bringing it All Together\n\nBack to the original expression:\n\n\n$$ P(User, Movie) = \n\\alpha_{U} \\times \\overline{R(User)} + \n\\alpha_{N} \\times \\overline{R(Movie)} + \nF_{SVD}(User, Movie| \\lambda, N_{Rank}) $$\n\nThis has two free parameters which can be tuned: $\\lambda$ and $N_{Rank}$.  $\\lambda$ controls how large SVD any row \nof the SVD matrix become and $N_{Rank}$ is the rank of the system.  We would like to optimize these two parameters.\n\nI did some manual tuning and found a good parameter set of $N_{Rank} = 30$ and $\\lambda = 1e-2$.\nThe class below is a self contained object which executes all the stages \nof the recommender system.", "cell_type": "markdown"}, {"execution_count": 14, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "class RecommenderSVD:\n    def __init__(self, loss_lambda, Nrank, \n                 rating_column = 'rating', user_column = 'user_id', \n                item_column = 'movie_id'):\n        self.loss_lambda = loss_lambda\n        self.Nrank = Nrank\n        self.rating_column = rating_column\n        self.user_column = user_column\n        self.item_column = item_column\n        self.linear_model = linear_model.LinearRegression(fit_intercept=False)\n        self.latent_mdl = linear_model.SGDRegressor(loss='squared_loss', penalty='l1', \n                                                    alpha =self.loss_lambda, fit_intercept=False,\n                                                   n_iter=100)\n\n\n    def train(self, df):\n        df_train = df.copy()\n        self.assign_group_means(df_train)\n        df_train['mean_user_rating'] = self.mean_user_rating.loc[df_train[self.user_column]].values\n        df_train['mean_item_rating'] = self.mean_item_rating.loc[df_train[self.item_column]].values\n        \n        ## Fit the linear model\n        self.fit_linear_model(df_train)\n        df_train['linear_prediction'] = self.predict_linear_model(df_train)\n\n        ## Fit the SVD model\n        df_train['linear_residuals'] = df_train['linear_prediction'] - df_train[self.rating_column]\n        self.fit_svd_model(df_train)\n        \n        \n    def predict(self, df):\n        df_test = df.copy()\n        df_test['mean_user_rating'] = self.mean_user_rating.loc[df_test[self.user_column]].values\n        df_test['mean_item_rating'] = self.mean_item_rating.loc[df_test[self.item_column]].values\n        \n        ## Predict the linear model\n        df_test['linear_prediction'] = self.predict_linear_model(df_test)     \n        \n        ## Predict svd model\n        df_test['svd_prediction'] = self.predict_svd_model(df_test)\n        return(df_test['svd_prediction'])\n    def fit_linear_model(self, df_train):\n        X = df_train.mean_item_rating - df_train.mean_user_rating\n        y = df_train[self.rating_column] - df_train.mean_user_rating\n        X = X.values.reshape(-1, 1)\n        self.linear_model.fit(X, y)\n    \n    def fit_svd_model(self, df):\n        tbl = df.pivot_table(values='linear_residuals', columns = self.item_column, \n                             index = self.user_column, aggfunc='sum', fill_value=0)\n        mtx = sparse.coo_matrix(tbl.as_matrix())\n\n        n_components = self.Nrank\n        U, S, V = la.svds(mtx, k=n_components)\n\n        df_users = pd.DataFrame(U.dot(np.diag(np.sqrt(S))), index=tbl.index)\n        df_items = pd.DataFrame(np.diag(np.sqrt(S)).dot(V).T, index=tbl.columns)\n        df_users.columns =  ['ULatent_%s'%str(ii).zfill(2) for ii in range(n_components)]\n        df_items.columns = ['MLatent_%s'%str(ii).zfill(2) for ii in range(n_components)]        \n        df_user_grp = df.groupby(self.user_column)\n        df_item_grp = df.groupby(self.item_column)\n        self.df_users_latent = df_users.apply(lambda row: self.fit_svd_latent_weights(row, row.name, \n                                                                                      'linear_residuals',\n                                                                                      df_items, df_user_grp, \n                                                                                      self.item_column), axis=1)\n        self.df_item_latent = df_items.apply(lambda row: self.fit_svd_latent_weights(row, \n                                                                                     row.name, 'linear_residuals',\n                                                                                     self.df_users_latent, \n                                                                                     df_item_grp, \n                                                                                     self.user_column), axis=1)\n    def fit_svd_latent_weights(self, latent_row, index, dependent_column,\n                               df_complimentary, df_grp, complimentary_column):\n        df_sub = df_grp.get_group(index)\n        X = df_complimentary.loc[df_sub[complimentary_column]]\n        y = df_sub[dependent_column]\n        self.latent_mdl.fit(X.as_matrix(), y.as_matrix())\n        coefs = np.array([cx for cx in self.latent_mdl.coef_])\n        df_results = pd.Series(coefs, index=latent_row.index.values)\n        return(df_results)\n\n\n    def predict_linear_model(self, df):\n        X = df.mean_item_rating - df.mean_user_rating\n        X = X.values.reshape(-1,1)\n        delta_p = self.linear_model.predict(X)\n        predict_rating = df.mean_user_rating + delta_p\n        return(predict_rating)\n    \n    def predict_svd_model(self, df):\n        p1 = self.df_users_latent.loc[df[self.user_column]]\n        p2 = self.df_item_latent.loc[df[self.item_column]]\n        p_prod = (p1.values * p2.values).sum(axis=1)\n        prediction = df['linear_prediction']-p_prod\n        return(prediction)\n\n    def group_means(self, df, column_name):\n        mean_vector = df.groupby(column_name)[self.rating_column].mean()\n        return(mean_vector)\n    \n    def assign_group_means(self, df):\n        self.mean_user_rating = self.group_means(df, self.user_column)\n        self.mean_item_rating = self.group_means(df, self.item_column)", "outputs": []}, {"execution_count": 15, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "mdl_svd = RecommenderSVD(1e-2, 30)\nmdl_svd.train(df_train)\nprediction = mdl_svd.predict(df_test)\nerr = df_test['rating'] - prediction\nrmse = np.sqrt((err**2).mean())\nprint(\"RMSE = %f\"%rmse)", "outputs": [{"output_type": "stream", "text": "RMSE = 0.866730\n", "name": "stdout"}]}, {"execution_count": 16, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "rounded_prediction = np.round(prediction)\nrounded_prediction[rounded_prediction < df_test.rating.min()] = df_test.rating.min()\nrounded_prediction[rounded_prediction > df_test.rating.max()] = df_test.rating.max()", "outputs": []}, {"execution_count": 17, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "from sklearn.metrics import confusion_matrix", "outputs": []}, {"execution_count": 18, "metadata": {"trusted": true, "collapsed": false}, "cell_type": "code", "source": "confusion = pd.DataFrame(confusion_matrix(df_test.rating, rounded_prediction), \n                         columns = ['Predition = %i'%ii for ii in range(1, 6)], \n                         index= ['Observed = %i'%ii for ii in range(1, 6)])\nHTML(confusion.to_html())", "outputs": [{"execution_count": 18, "metadata": {}, "output_type": "execute_result", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predition = 1</th>\n      <th>Predition = 2</th>\n      <th>Predition = 3</th>\n      <th>Predition = 4</th>\n      <th>Predition = 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Observed = 1</th>\n      <td>1678</td>\n      <td>6085</td>\n      <td>6743</td>\n      <td>2033</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>Observed = 2</th>\n      <td>459</td>\n      <td>6491</td>\n      <td>17521</td>\n      <td>7175</td>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>Observed = 3</th>\n      <td>138</td>\n      <td>4456</td>\n      <td>38226</td>\n      <td>34000</td>\n      <td>919</td>\n    </tr>\n    <tr>\n      <th>Observed = 4</th>\n      <td>25</td>\n      <td>1114</td>\n      <td>24758</td>\n      <td>73217</td>\n      <td>4900</td>\n    </tr>\n    <tr>\n      <th>Observed = 5</th>\n      <td>8</td>\n      <td>180</td>\n      <td>5528</td>\n      <td>47168</td>\n      <td>14932</td>\n    </tr>\n  </tbody>\n</table>"}}]}, {"metadata": {}, "source": "## Areas for improvement\n\n* **sklearn's** optimization routine is effective, but has some limitation in defining loss \nfunctions and optimization routines.  Other tools such as **Tensorflow** give more flexiblity, \nbut are not very inituative.  \n\n* Content based filter: This only looks at correlations between user's and movie's.  \nContent based filtering brings in more content focused results.  The data \ntends to be categorical in nature, so there is some processing which needs to \ntake place.  Tools like **pasty** are very useful in transforming these data types\n", "cell_type": "markdown"}, {"execution_count": null, "metadata": {"trusted": true, "collapsed": true}, "cell_type": "code", "source": "", "outputs": []}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.5.2", "name": "python"}, "_immuta": {"allow_rename": false}}, "nbformat_minor": 0}