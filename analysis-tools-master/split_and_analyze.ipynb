{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import predictionio\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ml_metrics as metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Data files are stored in hdfs\n",
    "# For convenience fool hdfs url is used \n",
    "# Url consists of four parts: base url, dataset name, dataset purpose (train/test) and version\n",
    "# It's supposed that fool dataset has .all extension\n",
    "#\n",
    "\n",
    "BASE_URL = ???\n",
    "DATASET_NAME = ???\n",
    "VERSION = \"1\"\n",
    "\n",
    "path_to_source_data = BASE_URL + \"/\" + DATASET_NAME + \".all\"\n",
    "path_to_train_data = BASE_URL + \"/\" + DATASET_NAME + \".train.\" + VERSION\n",
    "path_to_test_data = BASE_URL + \"/\" + DATASET_NAME + \".test.\" + VERSION\n",
    "\n",
    "#\n",
    "# Event list can be obtaned from data, but for reporting \n",
    "# purposes it's more convenient to define evens order.\n",
    "# Also primary event should be determined, the event we're\n",
    "# going to make prediction of.\n",
    "# And split event, this one will be used to determine \n",
    "# time moment for dataset split on train and test subsets.\n",
    "# It's often the same as primary event\n",
    "#\n",
    "\n",
    "PRIMARY_EVENT_NAME = ???\n",
    "SPLIT_EVENT_NAME = ??? # it should be often primary event\n",
    "eventsList = [???]\n",
    "\n",
    "report_file_prefix = DATASET_NAME + \".\" + VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json(path_to_source_data)\n",
    "df = df.withColumn(\"Date\", F.from_utc_timestamp(\"eventTime\", \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get number of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get events count\n",
    "df.groupBy('event').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_with_event_count = df.groupBy(F.col(\"entityId\").alias(\"user\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# How many users have only one event\n",
    "#\n",
    "\n",
    "users_with_event_count.filter(\"count = 1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Filter users with small number of events\n",
    "#\n",
    "\n",
    "min_events = 10\n",
    "users_with_few_events = (users_with_event_count\n",
    "                         .filter(\"count < %d\" % (min_events))\n",
    "                         .select(F.col(\"user\").alias(\"user_with_few_events\")))\n",
    "ndf = df.join(users_with_few_events, \n",
    "              F.col(\"entityId\")==F.col(\"user_with_few_events\"), \n",
    "              how=\"left_outer\")\n",
    "df1 = ndf.filter(\"user_with_few_events is NULL\").drop(\"user_with_few_events\")\n",
    "#df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check new number of records\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_split_date(df, train_ratio=0.8):\n",
    "    \"\"\"Calculates split date \n",
    "    \n",
    "    Calculates the moment of time that we will use to split \n",
    "    data into the train (befor the moment) and the test sets\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        train_ratio: ratio of samples in train set\n",
    "\n",
    "    Returns:\n",
    "        A datetime object        \n",
    "    \"\"\"\n",
    "    date_rdd = (df\n",
    "                .filter(\"event = '%s'\" % (PRIMARY_EVENT_NAME))\n",
    "                .select(\"Date\")\n",
    "                .sort(\"Date\", ascending=True)\n",
    "                .rdd)\n",
    "    total_primary_events = date_rdd.count()\n",
    "    split_date = (date_rdd\n",
    "                  .zipWithIndex()\n",
    "                  .filter(lambda x: x[1] > total_primary_events * train_ratio)\n",
    "                  .first()[0][0])\n",
    "    return split_date\n",
    "\n",
    "split_date = get_split_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No need in Date as we have eventTime\n",
    "# save data for training and then consequent test\n",
    "#WRIGHTING_MODE = \"error\" # or use mode=\"overwrite\"\n",
    "WRIGHTING_MODE = \"overwrite\"\n",
    "\n",
    "# Test records are newer then train so that we have no information leakage from the future\n",
    "df.filter(F.col(\"Date\") >= split_date).drop(\"Date\").write.json(path_to_test_data, mode=WRIGHTING_MODE) \n",
    "df.filter(F.col(\"Date\") < split_date).drop(\"Date\").write.json(path_to_train_data, mode=WRIGHTING_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now you may want to use your Spark cluster to perform the model training\n",
    "# Use pio import --appid <APPID> --input <path_to_train_data>\n",
    "# \n",
    "# You may continue analysis from the next line after save, just rerun 3 first lines\n",
    "# and copy split_date value to the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_date = datetime(2015, 12, 8, 1, 53, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We only need column subset for consequent steps\n",
    "#\n",
    "\n",
    "train_df = df.filter(F.col(\"Date\") < split_date).select(\"entityId\", \"event\", \"targetEntityId\").cache()\n",
    "test_df = df.filter(F.col(\"Date\") >= split_date).select(\"entityId\", \"event\", \"targetEntityId\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Calculation of different stat metrics of datasets\n",
    "#\n",
    "\n",
    "events_by_type = (df\n",
    "                  .groupBy(\"event\")\n",
    "                  .count()\n",
    "                  .select(F.col(\"event\"), F.col(\"count\").alias(\"count_total\"))\n",
    "                  .toPandas())\n",
    "\n",
    "events_by_type_test = (test_df\n",
    "                       .groupBy(\"event\")\n",
    "                       .count()\n",
    "                       .select(F.col(\"event\"), F.col(\"count\").alias(\"count_test\"))\n",
    "                       .toPandas()\n",
    "                       .set_index(\"event\"))\n",
    "\n",
    "events_by_type_train = (train_df\n",
    "                        .groupBy(\"event\")\n",
    "                        .count()\n",
    "                        .select(F.col(\"event\"), F.col(\"count\").alias(\"count_train\"))\n",
    "                        .toPandas()\n",
    "                        .set_index(\"event\"))\n",
    "\n",
    "# --- \n",
    "unique_users_by_event = (df\n",
    "                         .select(F.col(\"entityId\"), F.col(\"event\"))\n",
    "                         .distinct()\n",
    "                         .groupBy(\"event\")\n",
    "                         .count()\n",
    "                         .select(F.col(\"event\"), F.col(\"count\").alias(\"unique_users_total\"))\n",
    "                         .toPandas()\n",
    "                         .set_index(\"event\"))\n",
    "\n",
    "unique_users_by_event_train = (train_df\n",
    "                               .select(F.col(\"entityId\"), F.col(\"event\"))\n",
    "                               .distinct()\n",
    "                               .groupBy(\"event\")\n",
    "                               .count()\n",
    "                               .select(F.col(\"event\"), F.col(\"count\").alias(\"unique_users_train\"))\n",
    "                               .toPandas()\n",
    "                               .set_index(\"event\"))\n",
    "\n",
    "unique_users_by_event_test = (test_df\n",
    "                              .select(F.col(\"entityId\"), F.col(\"event\"))\n",
    "                              .distinct()\n",
    "                              .groupBy(\"event\")\n",
    "                              .count()\n",
    "                              .select(F.col(\"event\"), F.col(\"count\").alias(\"unique_users_test\"))\n",
    "                              .toPandas()\n",
    "                              .set_index(\"event\"))\n",
    "\n",
    "# --- \n",
    "unique_items_by_event = (df\n",
    "                         .select(F.col(\"targetEntityId\"), F.col(\"event\"))\n",
    "                         .distinct()\n",
    "                         .groupBy(\"event\")\n",
    "                         .count()\n",
    "                         .select(F.col(\"event\"), F.col(\"count\").alias(\"unique_items_total\"))\n",
    "                         .toPandas()\n",
    "                         .set_index(\"event\"))\n",
    "\n",
    "unique_items_by_event_train = (train_df\n",
    "                               .select(F.col(\"targetEntityId\"), F.col(\"event\"))\n",
    "                               .distinct()\n",
    "                               .groupBy(\"event\")\n",
    "                               .count()\n",
    "                               .select(F.col(\"event\"), F.col(\"count\").alias(\"unique_items_train\"))\n",
    "                               .toPandas()\n",
    "                               .set_index(\"event\"))\n",
    "\n",
    "unique_items_by_event_test = (test_df\n",
    "                              .select(F.col(\"targetEntityId\"), F.col(\"event\"))\n",
    "                              .distinct()\n",
    "                              .groupBy(\"event\")\n",
    "                              .count()\n",
    "                              .select(F.col(\"event\"), F.col(\"count\").alias(\"unique_items_test\"))\n",
    "                              .toPandas()\n",
    "                              .set_index(\"event\"))\n",
    "\n",
    "# totals\n",
    "events = df.count()\n",
    "events_train = train_df.count()\n",
    "events_test = test_df.count()\n",
    "\n",
    "unique_users = df.select(\"entityId\").distinct().count()\n",
    "unique_users_train = train_df.select(\"entityId\").distinct().count()\n",
    "unique_users_test = test_df.select(\"entityId\").distinct().count()\n",
    "\n",
    "unique_items = df.select(F.col(\"targetEntityId\")).distinct().count()\n",
    "unique_items_train = train_df.select(F.col(\"targetEntityId\")).distinct().count()\n",
    "unique_items_test = test_df.select(F.col(\"targetEntityId\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info_df = events_by_type\n",
    "dfs = [events_by_type_train, events_by_type_test, \n",
    "       unique_users_by_event, unique_users_by_event_train, unique_users_by_event_test, \n",
    "       unique_items_by_event, unique_items_by_event_train, unique_items_by_event_test]\n",
    "\n",
    "for data_frame in dfs:\n",
    "    info_df = info_df.join(data_frame, on=\"event\")\n",
    "    \n",
    "n_rows, n_cols = info_df.shape\n",
    "\n",
    "# totals\n",
    "info_df.loc[n_rows] = ['ANY EVENT', events, events_train, events_test, \n",
    "                  unique_users, unique_users_train, unique_users_test, \n",
    "                  unique_items, unique_items_train, unique_items_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info_df.to_csv(report_file_prefix + \"_split_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mk_intersection_matrix(by_rows, columns_for_matrix, \n",
    "                           horizontal_suffix=\"\", vertical_suffix=\"\"):\n",
    "    \"\"\" Makes pandas dataframe of intersections out of list of rows\n",
    "    \n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(columns=[col + horizontal_suffix for col in columns_for_matrix])\n",
    "    for en in columns_for_matrix:\n",
    "        result.loc[en + vertical_suffix, :] = [0] * len(columns_for_matrix)\n",
    "    for r in by_rows:\n",
    "        row = r.asDict()\n",
    "        en_h = row['event_left']\n",
    "        en_v = row['event_right']\n",
    "        count = row['count']\n",
    "        result.loc[en_v + vertical_suffix, en_h + horizontal_suffix] = count\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_for_matrix = eventsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train_users = (\n",
    "    train_df\n",
    "    .select(F.col(\"entityId\").alias(\"user\"), F.col(\"event\").alias(\"event_left\"))\n",
    "    .distinct()\n",
    "    .join(train_df.select(F.col(\"entityId\").alias(\"user\"), F.col(\"event\").alias(\"event_right\")).distinct(), \n",
    "       on=\"user\", how=\"inner\")\n",
    "    .groupBy([\"event_left\", \"event_right\"])\n",
    "    .count()\n",
    "    .collect())\n",
    "\n",
    "trtru = mk_intersection_matrix(train_train_users, columns_for_matrix)\n",
    "trtru.to_csv(report_file_prefix + \"_train_train_user_intersection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_users = (\n",
    "    train_df\n",
    "    .select(F.col(\"entityId\").alias(\"user\"), F.col(\"event\").alias(\"event_left\"))\n",
    "    .distinct()\n",
    "    .join(test_df.select(F.col(\"entityId\").alias(\"user\"), F.col(\"event\").alias(\"event_right\")).distinct(), \n",
    "       on=\"user\", how=\"inner\")\n",
    "    .groupBy([\"event_left\", \"event_right\"])\n",
    "    .count()\n",
    "    .collect())\n",
    "\n",
    "\n",
    "trtsu = mk_intersection_matrix(train_test_users, columns_for_matrix, \n",
    "                               horizontal_suffix=\" train\", vertical_suffix=\" test\")\n",
    "trtsu.to_csv(report_file_prefix + \"_train_test_user_intersection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train_items = (\n",
    "    train_df\n",
    "    .select(F.col(\"targetEntityId\").alias(\"item\"), F.col(\"event\").alias(\"event_left\"))\n",
    "    .distinct()\n",
    "    .join(train_df.select(F.col(\"targetEntityId\").alias(\"item\"), F.col(\"event\").alias(\"event_right\")).distinct(), \n",
    "       on=\"item\", how=\"inner\")\n",
    "    .groupBy([\"event_left\", \"event_right\"])\n",
    "    .count()\n",
    "    .collect())\n",
    "\n",
    "trtri = mk_intersection_matrix(train_train_items, columns_for_matrix)\n",
    "trtri.to_csv(report_file_prefix + \"_train_train_item_intersection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_items = (\n",
    "    train_df\n",
    "    .select(F.col(\"targetEntityId\").alias(\"item\"), F.col(\"event\").alias(\"event_left\"))\n",
    "    .distinct()\n",
    "    .join(test_df.select(F.col(\"targetEntityId\").alias(\"item\"), F.col(\"event\").alias(\"event_right\")).distinct(), \n",
    "       on=\"item\", how=\"inner\")\n",
    "    .groupBy([\"event_left\", \"event_right\"])\n",
    "    .count()\n",
    "    .collect())\n",
    "\n",
    "trtsi = mk_intersection_matrix(train_test_items, columns_for_matrix,\n",
    "                               horizontal_suffix=\" train\", vertical_suffix=\" test\")\n",
    "trtsi.to_csv(report_file_prefix + \"_train_test_item_intersection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Now we perform \"dummy test\"\n",
    "# We evaluate performance of \"naive\" predictors and use them as a baseline.\n",
    "#\n",
    "# Three baseline MAP @ k experiments:\n",
    "# 1. Random sampling from items (uniform) - i.e. naive predicor with the list\n",
    "# of items and as it's naive it just predicts some item from its list \n",
    "#\n",
    "# 2. Random sampling from items (according to their distribution in training data)\n",
    "# In this case the chance of choosing the item is proportional to the item popularity\n",
    "#\n",
    "# 3. Top-N items from training data\n",
    "# This predictor always use most popular items as its predictions\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Items counts\n",
    "#\n",
    "counts = train_df.filter(\"event = '%s'\" % (PRIMARY_EVENT_NAME)).groupBy(\"targetEntityId\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_rating = sorted([(row.asDict()['count'], row.asDict()['targetEntityId']) for row in counts], reverse=True)\n",
    "elements = np.array([item for cnt, item in sorted_rating])\n",
    "probs = np.array([cnt for cnt, item in sorted_rating])\n",
    "probs = 1.0 * probs / probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_map_test_dummy(data, items=None, probs=None, uniform=True, top=True,\n",
    "                       users=None, primaryEvent=PRIMARY_EVENT_NAME, K=10, no_progress=False):\n",
    "    \"\"\"Performs dummy test\n",
    "    \n",
    "    Args:\n",
    "        data: list of event rows\n",
    "        items: np.array or list of items sorted in descending popularity order\n",
    "        probs: np.array or list of corresponding probabilities (needed for experiment #2)\n",
    "        uniform: Boolean flag to use uniform sampling\n",
    "        top: Boolean flag to use top items\n",
    "        users: set of users to consider\n",
    "        primaryEvent: str name of primary event\n",
    "        K: int for MAP @ K\n",
    "        no_progress: Boolean flag not to show the progress bar during calculations\n",
    "    \n",
    "    Returns:\n",
    "        list of [MAP@1, MAP@2, ... MAP@K] evaluations\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for rec in data:\n",
    "        if rec.event == primaryEvent:\n",
    "            user = rec.entityId\n",
    "            item = rec.targetEntityId\n",
    "            if (users is None) or (user in users):\n",
    "                d.setdefault(user, []).append(item)\n",
    "    \n",
    "    holdoutUsers = d.keys()\n",
    "    \n",
    "    prediction = []\n",
    "    ground_truth = []\n",
    "    if no_progress:\n",
    "        gen = holdoutUsers\n",
    "    else:\n",
    "        gen = tqdm(holdoutUsers)\n",
    "    for user in gen:\n",
    "        if top:\n",
    "            test_items = items[0:K]\n",
    "        elif uniform:\n",
    "            test_items = np.random.choice(items, size=(K,))\n",
    "        else:\n",
    "            test_items = np.random.choice(items, size=(K,), p=probs)\n",
    "        prediction.append(test_items)\n",
    "        ground_truth.append(d.get(user, []))\n",
    "    return [metrics.mapk(ground_truth, prediction, k) for k in range(1, K + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_df.filter(\"event = '%s'\" % (PRIMARY_EVENT_NAME)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# case 1. Random sampling from items (uniform)\n",
    "run_map_test_dummy(test_data, items=elements, probs=probs, uniform=True, top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# case 2. Random sampling from items (according to their distribution in training data)\n",
    "run_map_test_dummy(test_data, items=elements, probs=probs, uniform=False, top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# case 3. Top-N items from training data\n",
    "run_map_test_dummy(test_data, items=elements, probs=probs, uniform=True, top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Additional test to chech top-20 most popular items \n",
    "# what MAP@1 score do they give\n",
    "#\n",
    "\n",
    "for i in range(20):\n",
    "    r = run_map_test_dummy(test_data, items=elements[i:], uniform=True, top=True, K=1, no_progress=True)[0]\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# MAP test itself\n",
    "# One should have trained model and pio deploy running to perform MAP test\n",
    "# of the model\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "test_df = sqlContext.read.json(path_to_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = (test_df\n",
    "             .filter(\"event = '%s'\" % (PRIMARY_EVENT_NAME))\n",
    "             .select(\"entityId\", \"event\", \"targetEntityId\")\n",
    "             .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_map_test(data, eventNames, users=None, primaryEvent=PRIMARY_EVENT_NAME, \n",
    "                 consider_non_zero_scores=True, num=100, K=10,\n",
    "                 test=False, predictionio_url=\"http://0.0.0.0:8000\"):\n",
    "    N_TEST = 2000\n",
    "    d = {}\n",
    "    res_data = {}\n",
    "    engine_client = predictionio.EngineClient(url=predictionio_url)\n",
    "\n",
    "    for rec in data:\n",
    "        if rec.event == primaryEvent:\n",
    "            user = rec.entityId\n",
    "            item = rec.targetEntityId\n",
    "            if (users is None) or (user in users):\n",
    "                d.setdefault(user, []).append(item)\n",
    "    \n",
    "    if test:\n",
    "        holdoutUsers = d.keys()[1:N_TEST]\n",
    "    else:\n",
    "        holdoutUsers = d.keys()\n",
    "    \n",
    "    prediction = []\n",
    "    ground_truth = []\n",
    "    user_items_cnt = 0.0\n",
    "    users_cnt = 0\n",
    "    for user in tqdm(holdoutUsers):\n",
    "        q = {\n",
    "            \"user\": user,\n",
    "            \"eventNames\": eventNames,\n",
    "            \"num\": num,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            res = engine_client.send_query(q)\n",
    "            # Sort by score then by item name\n",
    "            tuples = sorted([(r[\"score\"], r[\"item\"]) for r in res[\"itemScores\"]], reverse=True)\n",
    "            scores = [score for score, item in tuples]\n",
    "            items = [item for score, item in tuples]\n",
    "            res_data[user] = {\n",
    "                \"items\": items,\n",
    "                \"scores\": scores,\n",
    "            }\n",
    "            # Consider only non-zero scores \n",
    "            if consider_non_zero_scores:\n",
    "                if len(scores) > 0 and scores[0] != 0.0:\n",
    "                    prediction.append(items)\n",
    "                    ground_truth.append(d.get(user, []))\n",
    "                    user_items_cnt += len(d.get(user, []))\n",
    "                    users_cnt += 1\n",
    "            else:\n",
    "                prediction.append(items)\n",
    "                ground_truth.append(d.get(user, []))\n",
    "                user_items_cnt += len(d.get(user, []))\n",
    "                users_cnt += 1\n",
    "        except predictionio.NotFoundError:\n",
    "            print(\"Error with user: %s\" % user)\n",
    "    return ([metrics.mapk(ground_truth, prediction, k) for k in range(1, K + 1)], \n",
    "            res_data, user_items_cnt/users_cnt)\n",
    "\n",
    "# \n",
    "def get_nonzero(r_data):\n",
    "    users = [user for user, res_data in r_data.items() if res_data['scores'][0] != 0.0]\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Primary event test\n",
    "# This test is mostly necessary to find users with non-zero item scores\n",
    "#\n",
    "(map_res, res_data, items_per_user) = run_map_test(test_data, [PRIMARY_EVENT_NAME], test=False, num=2)\n",
    "map_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# use this number rounded as the parameter K in run_map_test\n",
    "#\n",
    "\n",
    "items_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_zero_users = get_nonzero(res_data)\n",
    "len(non_zero_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_zero_users_csv = \"non_zero_users.\" + DATASET_NAME + \".\" + VERSION + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=non_zero_users, \n",
    "             columns=['user']).to_csv(non_zero_users_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore non zero users if rerun\n",
    "#non_zero_users = set(pd.read_csv(non_zero_users_csv)['user'].astype(np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Test every event separately\n",
    "# \n",
    "\n",
    "runResult = {}\n",
    "for ev in eventList:\n",
    "    (r_scores, r_data, ipu) = run_map_test(test_data, [ev], users=set(non_zero_users), K=3, test=False)\n",
    "    runResult[ev] = {\n",
    "        \"r_scores\": r_scores,\n",
    "        #\"r_data\": r_data\n",
    "    }\n",
    "    print(ev)\n",
    "    print(r_scores)\n",
    "    print(len(get_nonzero(r_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# All events\n",
    "#\n",
    "evl = eventsList\n",
    "r_scores, r_data, ipu = run_map_test(test_data, evl, users=set(non_zero_users), K=3, test=False)\n",
    "print(r_scores)\n",
    "print(len(get_nonzero(r_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# All but...\n",
    "#\n",
    "for ev in eventList:\n",
    "    evs = list(eventsList)\n",
    "    evs.remove(ev)\n",
    "    (r_scores, r_data, ipu) = run_map_test(test_data, evs, users=set(non_zero_users), K=3, test=False)\n",
    "    runResult[ev] = {\n",
    "        \"r_scores\": r_scores,\n",
    "        #\"r_data\": r_data\n",
    "    }\n",
    "    print(ev)\n",
    "    print(r_scores)\n",
    "    print(len(get_nonzero(r_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pairs\n",
    "for i in range(len(eventsList)):\n",
    "    for j in range(i + 1, len(eventsList)):\n",
    "        event_pair = [eventsList[i], eventsList[j]]\n",
    "        (r_scores, r_data, ipu) = run_map_test(test_data, event_pair,\n",
    "                                               users=set(non_zero_users), K=3, test=False)\n",
    "        print(event_pair)\n",
    "        print(r_scores)\n",
    "        print(len(get_nonzero(r_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
